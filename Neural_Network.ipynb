{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    z = 1/(1+np.exp(-x))\n",
    "    return z,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    z = max(0,x)\n",
    "    return z,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layer_dimensions):\n",
    "    np.random.seed(1)\n",
    "    params ={}\n",
    "    n_l = len(layer_dimensions)\n",
    "\n",
    "    for i in range (1, n_l): \n",
    "        params[\"W\" + str(i)] = np.random.randn(layer_dimensions[i], layer_dimensions[i-1])*0.01\n",
    "        params[\"b\" + str(i)] = np.zeros((layer_dimensions[i], 1))\n",
    "\n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_linear(A,W,b):\n",
    "    Z = np.dot(W,A) + b \n",
    "    back_prop_mem=(A,W,b)\n",
    "    return Z,back_prop_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_single(A_prev, W,b,activation):\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        Z,linear_cache = forward_prop_linear(A_prev,W,b)\n",
    "        A,act_cache = relu(Z)\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z,linear_cache = forward_prop_linear(A_prev,W,b)\n",
    "        A,act_cache = sigmoid(Z)\n",
    "\n",
    "    back_prop_mem = (linear_cache, act_cache)\n",
    "\n",
    "    return A, back_prop_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,params):\n",
    "    back_prop_mem = []\n",
    "    A = X\n",
    "    n_l = len(params)//2\n",
    "\n",
    "    for i in range (1,n_l):\n",
    "        A_prev = A \n",
    "        A,cache=forward_prop_single(A_prev,params[\"W\"+str(i)], params[\"b\"+str(i)], \"relu\")\n",
    "        back_prop_mem.append(cache)\n",
    "    \n",
    "    Y_pred,cache = forward_prop_single(A_prev,params[\"W\"+str(i)], params[\"b\"+str(i)], \"sigmoid\")\n",
    "    back_prop_mem.append(cache)\n",
    "\n",
    "    return Y_pred,back_prop_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y_pred,Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)*(np.sum(np.multiply(Y,np.log(Y_pred))+np.multiply(1-Y,np.log(Y_pred))))\n",
    "    cost=np.squeeze(cost)\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads(dZ,cache):\n",
    "    A_prev,W,b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW =(1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*(np.sum(dZ,axis=1,keepdims=True))\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev,dW,db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
